Q：词向量和句向量是一回事吗？RAG计算相似度是用词向量还是句向量呀
向量 是embedding，你可以用于 word embedding, 也可以用于 sentence embedding，当然也可以用于  chunk embedding
input内容 => 会将这个内容进行向量化，即 embedding

RAG计算的是什么？
RAG计算的是 chunk embedding，因为放到向量数据库中，是以 chunk（切分）作为基本的单元。
 
chunk 一般是300-500，也可能是1000个token

Q：句向量是怎么来的？上次讲的embeding过程，神经网络的输入和输出都是词吧
sentence embedding = 这个句子中的 word embedding的加权平均
上次讲的embeding过程，神经网络的输入 = 单词
输出 = 单词（作为邻居）的概率

Q：怎么把调用模型API改为服务器部署好的大模型，另外知识库单个文件改为文件夹
现在使用的是 dashscope 接口，我们也可以部署自己的大模型，通过 http serivce，按照dashscope/openai 接口进行返回内容
server填写自己的 api url即可

faiss 是向量数据库，你可以写一个函数，遍历某个文件夹，将里面所有的文件，放到faiss中进行索引；

Q：负向量是什么？
负样本
正样本 就是 正确的，你想要找的样本
负样本 就是 非正样本

Q：有否定向量的数据库吗？稀疏向量
向量数据库，是按照embedding相似检索，embedding 是通过窗口共现计算出来的。其实统计是邻居的词频的概率

Q：比如我不想要什么数据，不想要，就是否定的意思。。。 这种好像没有
唐僧不是妖怪 => 转化为 正向的语句，即 唐僧是人类

ChatPDF-faiss 有几个模型：
1）embedding模型 => 计算文本向量相似度，进行文本chunk的筛选
2) LLM模型 => 回答用户问题
LLM模型 >> embedding模型
Qwen3-embedding 0.6B 到 8B不等；

Q：qwen3-embedding 想fasis一样，存储需要我们自己处理吗？
存储faiss就可以直接帮你处理，你需要把 编码规则告诉faiss

Q: 模型后面的几B是什么意思？根据什么命名的？
B 是英文单词 10亿的缩写，所以8B 就是80亿的参数的大模型


如果直接RAG => 模型可以给出正确答案，准确率还很高 => 还需要进行 RAG微调么？=> 不需要了

因为我们就算是在开卷的考试中，给你了一些参考 => LLM + RAG 还是没有回答对，或者说准确率不高 => 如果RAG包括了 正确答案，说明LLM的推理能力不够
=> 有可能RAG中，会有一些干扰项
embedding 是向量相似度检索，不一定这些embedding内容 都是和答案相关
=> 需要训练模型的能力，来区分这些开卷考试中，提供的chunks, 让LLM可以更关注到那些有价值的chunks


目前很多企业的RAG项目，还处于 native rag阶段 => 
还是灌一些基础的文档，然后做简单的切分 和 embedding的设置，就想让LLM给你正确答案
=> 用户提了一些问题，答案回答的不是很好

闭卷 => 相当于将知识存储到大模型中，如果是一些领域的基本概念，放到大模型中是有价值的！
但是如果是一些灵活的问题，一些政策（这些政策可能会变的）

Q：RAFT是调整整个大模型的weights? 还是局部weights?
1）全量微调 => 成本高，效果好
2）高效微调 => 效率比较高，LoRA

Q: 觉得很神奇， 调了之后没有RAG也提升performance
微调，相当于把一些重要的概念、知识，存储到了大模型中，让它提前进行了训练

我们的考试是可以开卷，也可以闭卷考试。 有些能力强的人，闭卷考试成绩  > 其他人的开卷考试

Lora进行高效微调，unsloth

Faiss的编码规则，其实就是你选择的embedding的编码规则；Faiss只是向量数据库，它不指定编码规则。bge-m3 你可以指定对应的embedding规则

word2vec，可以指定  vec_size，是人工指定的；
vec_size越大 => 可以包括的精细化的程度，就越好

qwen3-embedding
https://modelscope.cn/models/Qwen/Qwen3-Embedding-8B


Q：RAG的过程中，用相似度计算命中问题，是否会得到错误答案，比如如果语料库是西游记，我的问题是谁不是孙悟空的师傅，是否会命中唐僧

孙悟空、师傅 => 确实召回的chunk，很有可能会有唐僧，比如chunk是这么写：唐僧是孙悟空的师傅……
LLM  + chunk => 

Q: 如果要换新的嵌入模型，向量数据库中的历史数据是不是需要重新做embedding？
原文不变，但是embedding变化了，那么faiss的内容会不会变化？ => 会变化的

Q：整个kimi和chatGPT是一样的作用吗？还是自己可以独立设计的助手工具？
我一般用kimi，是把kimi当成搜索引擎 + 整理

Q：如果问题相似性！=问题相关性了，召回结果中没有正确答案，那大模型也无
法回答了？这种问题是无法解决的吗
存在这样的概率，但是 如果你问的问题是 否定的

LLM 懂否定的，你问的问题，如果是： 谁不是孙悟空的师傅 => LLM一般都懂的
你可以通过 TopK，把K设置的大一些，这样就可以召回更多的有效的chunks，然后让LLM来筛选和回答


召回的多一些 + Rerank筛选

Q：既然已经召回topN了，为什么不直接让大模型判断，还要 rerank
TopN，N是否可以比较大？
如果N=50呢？=> 不能直接放到 LLM中
如果N比较小 => 不需要进行Rerank

rerank 比LLM更经济，也可以保证 LLM的上下文窗口，可以放得下；
rerank 会过滤掉哪些不太有价值的chunk，就不需要让LLM进行推理；
如果你有rerank算法了，是不是可以在召回阶段，多设置一些 TopN

RAG的最佳实践里面，Rerank可以提分，提升9%的准确率；

Q：rerank类似，面试50选5
召回类似简历的筛选，模型比较粗，速度比较快，可以筛选大量的简历出来。
比如 10万个chunks => 100个chunk 作为候选
rerank会更精细化一些，对这100个chunk进行面试 => Top5

Q：rerank 模型能理解语义吗？比如之前说的否定类问题
会比召回阶段好很多，因为这个模型会更精细。它会从语义上面进行打分
召回embedding能力 < rerank embedding能力 < LLM能力

Q：rerank最终选几个也是人为指定么
是的

Q：有双向改写的代码示例吗？程序是怎么做到灵活的双向改写问题文档的？
可以通过大模型来完成
RAG是一个工程，里面可以有很多的策略

LLM prompt：你是一个查询改写的AI，将用户提问的问题先进行回答，可以拆解成不同的维度，方便后面针对你回答的内容 进行知识库的检索

Small to Big
生成摘要，生成 query2doc, 生成 doc2query，都可以用LLM

RAG是一个系统工程，而是一堆小工具的叠加。

RAG，Chunk, Small to Big

这个方法NB，先用大模型清洗了原始数据, 然后再embedding，准确率肯定更好

Q : 大模型在整个流程中是复用的吗
大模型可以作为工具，在RAG过程中进行使用，帮你制作摘要、生成新的问题等；

在RAG流程中，我们需要有一些工作要做。LLM就是 “管培生”，你可以把工作，通过prompt告诉它

Q：老师提到的策略都是可以用大模型来做把
是的，很多策略都可以用LLM prompt，让大模型来完成；

Q：做rag相关的工作，一般用多大规模的llm合适呢？
如果是个人，一般用7B大模型就可以了，因为更大的模型，GPU成本高；
如果是公司，一般会32B，72B，质量好；
如果是API接口，你可以用 性价比高的，比如 qwen-turbo-latest

Q：small-to-big ，就是对大模型的多轮提问吗？如果是的话，对话很随机，怎么控制呢？
small 是对文档的摘要生成；prompt是可以固定的

Q：72B需要多大显存？
7B 我会用一块RTX 4090，72B，我会用 10块 RTX 4090

Q：rerank模型需要gpu吗？cpu上能快速返回结果吗
最好用gpu，cpu可以返回结果，但是速度慢，如果只是做一次，没有问题

知识图谱，会有很多实体，实体可以是一个人名、地名
赵云、周瑜、吕布 
三国演义里面会有很多英雄，实体和实体之间是不是有关系；
周瑜 和 小乔；
周瑜的夫人的父亲是谁？

native rag，仅仅是基于chunk，了解的信息很片段，很难连成一个整体；

Q：用来考研也不错啊
如果你把考研知识，整理的很详细，graphRAG整理成知识图谱，那么本身就很有价值
用户提了问题之后，可以在整个社区（知识体系）中进行回答

打卡1：GraphRAG （三国演义 three_kingdoms）option
打卡2：Qwen-Agent 建议打卡

Q：这个内容可以图形化显示吗？
graphrag 这个项目应该是不行，你可以使用 neo4j，加上这些内容，进行可视化显示

Q：这个和notebooklm比怎么样
notebooklm 是类似native rag + 优化，速度快、效果还不错。
如果你想要更全面的，可以用graphrag，适合完整的体系，比如一本小说，考研（知识entity之间是有联系的）

Q：老师：回答问题的时候还需要消耗token吗
消耗

qwen-agent 不仅可以做RAG，还可以做 工具调用

钉钉助手，类似Coze，可以快速搭建RAG问答；chatpdf-faiss
OA流程助手，钉钉助手回答的质量不是特别好；
qwen-agent来回答RAG，质量 比钉钉助手要好；

Q：是不是很费token
相比于chatpdf-faiss，是消耗更多的token
但是比graphrag，消耗的token要少很多

Q：这个qwen agent 把文档存储在向量库还是其他哪里
在workspace下面

Q：qwen agent兼容不同的语言查询码
主要是中文和英文
主要是生成的关键词 能否和原文的语言匹配上；

Q：老师可以对比一下qianwen_agent 与 notebookLM 与 Graphrag等多种方法的
优劣吗
notebookLM 是闭源的产品，好用，但是不公开
qwen-agent 是开源的产品，好用，性价比高，速度不错，质量还可以
graphrag 是开源的产品，速度慢，但是质量好（解析的范围更全面）

如果是追求全面，研究性的，不考虑价格、时间成本 =>  graphrag
如果是追求性价比，回答质量不错，成本可控，速度快 => qwen-agent
如果是直接用现成的，不写代码，质量又要很好 => notebookLM

Q：企业级能用qwen-agent吗
可以，因为是开源的
需要配置的是大模型，大模型也可以用本地的大模型

Q：GraphRAG给到LLM的input或prompt是什么
input是 文章，prompt在开源项目中 prompt文件夹

Q：Qwen-Agent就是一串有序的提示词？
有很多策略，三级RAG步骤
agent rag，自主性的RAG

Q：我有一系列比较相关的国家标准，都是PDF的，他们直接是有联系性的。我是把这些合并了让agent分析，还是让他读取很多歌pdf
可以分开，放到files中，让它看到很多pdf

Q：GraphRAG可以直接用本地的大模型吗？
理论上是可以的，因为代码是开源的，可能要自己调整下代码

Q：千问agent可以换模型么
一般用dashscope里面的模型会更方便，也可以换模型

